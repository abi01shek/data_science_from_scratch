from functools import partial 

def difference_quotient(f,x,h):
    return (f(x+h)-f(x))/h

def square(x):
    return x*x

def derivative_sq(x):
    return 2*x

derivative_estimate = partial(difference_quotient,square,h=0.00001)

# import matplotlib.pyplot as plt
# x = range(-10,10)
# plt.plot(x, list(map(derivative_sq,x)),'rx',label='Actual')
# plt.plot(x, list(map(derivative_estimate,x)), 'b+', label='Estimate')
# plt.legend(loc=9)
# plt.show()


def some_func(v):
    ''' some function with two variables '''
    return (5*v[0]**2+4*v[1])



def partial_difference_quotient(f,v,i,h):
    ''' increment ith variable by h and calculate partial difference '''
    w = [v_j+(h if (i==j) else 0)
         for j,v_j in enumerate(v)]
    return (f(w)-f(v))/h

def estimate_gradient(f,v,h=0.00001):
    return [partial_difference_quotient(f,v,i,h)
            for i,_ in enumerate(v)]


# v = [2,3] #x=2, y=3
# # h = 0.00001 
# # i = 0 #partial derivative wrt x
# # print(partial_difference_quotient(some_func,v,i,h))
# print(estimate_gradient(some_func,v))


def step(v, direction, step_size):
    """ move step_size in direction from v """
    return [v_i + step_size*direction_i
            for v_i, direction_i in zip(v,direction)]

def sum_of_squares_gradient(v):
    return [ 2*v_i
             for v_i in v]


import math
def distance(v,w):
    return math.sqrt(sum([(v_i-w_i)**2 for v_i, w_i in zip(v,w)]))

#pick a random starting point
import numpy as np
v = [np.random.randint(-10,10) for i in range(3)] #3 variables

tolerance = 0.0000001

while True:
    gradient = sum_of_squares_gradient(v)
    next_v = step(v, gradient, -0.01) #move in direction opposite to gradient hence negative
    if(distance(next_v,v) < tolerance):
        break
    v = next_v

print(v)


def safe(f):
    """ return a new function that is same as f except that
    it outputs infinity whenever f produces an error"""
    """ infinity cannot be less than anything """
    def safe_f(*args, **kwargs):
        try:
            return f(*args, **kwargs)
        except:
            return float('inf')
        return safe_f



## Putting it all together
def target_fn(v):
    return v[0]**3+v[1]**2+v[0]

def gradient_fn(v):
    return [3*v[0]**2, 2*v[1], 1]

##random starting point
theta_0 = [np.random.randint(-10,10) for i in range(3)]

def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.0000001):
    step_sizes = [100,10,1,0.1,0.01,0.0001,0.00001]
    theta = theta_0
    target_fn = safe(target_fn)
    value = target_fn(theta)

    while True:
        gradient = gradient_fn(theta)
        next_thetas = [step(theta, gradient, -step_size) for step_size in step_sizes]

        #choose the one that minimizes error function
        next_theta = min(next_thetas,key=target_fn)
        next_value = target_fn(next_theta)

        #stop if we are converging
        if abs(value - next_value) < tolerance:
            return theta
        else:
            theta, value = ne
